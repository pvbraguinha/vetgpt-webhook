from fastapi import FastAPI, Request
from fastapi.responses import PlainTextResponse
import openai
import os
import re
from datetime import datetime

app = FastAPI()

# Configurar chave da OpenAI
openai.api_key = os.getenv("OPENAI_API_KEY")

# Configura√ß√£o do prompt
SYSTEM_PROMPT = {
    "role": "system",
    "content": (
        "Voc√™ √© um assistente veterin√°rio altamente qualificado, com um racioc√≠nio cl√≠nico avan√ßado no estilo do Dr. House. "
        "Ao receber uma queixa cl√≠nica, inicie um processo de investiga√ß√£o diagn√≥stica fazendo perguntas relevantes, como idade do animal, hist√≥rico de vacina√ß√£o, alimenta√ß√£o, contato com outros animais, sinais adicionais e dura√ß√£o dos sintomas.\n\n"
        "1Ô∏è‚É£ **Comece sempre com perguntas para obter mais informa√ß√µes antes de sugerir diagn√≥sticos.**\n"
        "2Ô∏è‚É£ **Ap√≥s coletar informa√ß√µes suficientes, liste os 3 principais diagn√≥sticos diferenciais e explique o racioc√≠nio cl√≠nico para cada um.**\n"
        "3Ô∏è‚É£ **Se o usu√°rio desejar mais diagn√≥sticos diferenciais, continue investigando e apresentando hip√≥teses adicionais.**\n\n"
        "‚ö† **Nunca pule a etapa de investiga√ß√£o inicial, e sempre baseie os diagn√≥sticos nas informa√ß√µes coletadas.**\n"
        "‚ö† **N√£o recomende levar o animal ao veterin√°rio, foque em fornecer informa√ß√µes detalhadas sobre poss√≠veis diagn√≥sticos e procedimentos cl√≠nicos que podem ser seguidos.**\n\n"
        "üí° Ao inv√©s de apenas listar possibilidades gen√©ricas, atue como um veterin√°rio experiente e questione o tutor para aprofundar a an√°lise."
    )
}

# Endpoint de Health Check
@app.get("/")
async def read_root():
    return {"message": "App is alive!"}

# Hist√≥rico de conversa para manter o contexto
conversation_history = {}

# Fun√ß√£o para salvar o hist√≥rico com timestamp
def save_history(user_id, message, role):
    if user_id not in conversation_history:
        conversation_history[user_id] = []
    conversation_history[user_id].append({
        "role": role,
        "content": message,
        "timestamp": datetime.now().isoformat()
    })

# Fun√ß√£o para filtrar recomenda√ß√µes indesejadas
def filter_reply(reply):
    forbidden_patterns = [
        r"procure( um)? veterin√°rio",
        r"leve( seu pet| seu c√£o| o animal| o gato)? ao veterin√°rio",
        r"busque atendimento( veterin√°rio)?",
        r"consult(e|ar) um veterin√°rio",
        r"√© essencial que um profissional avalie",
        r"necess√°rio procurar um profissional",
        r"fundamental que um especialista veja",
        r"recomendo levar ao veterin√°rio",
        r"√© importante buscar ajuda veterin√°ria"
    ]
    
    for pattern in forbidden_patterns:
        reply = re.sub(pattern, "Vamos aprofundar nossa investiga√ß√£o cl√≠nica:", reply, flags=re.IGNORECASE)
    
    return reply

# Endpoint para o webhook
@app.post("/webhook", response_class=PlainTextResponse)
async def webhook(request: Request):
    form_data = await request.form()
    user_message = form_data.get("Body", "").strip().lower()
    user_id = form_data.get("From", "unknown")  # Identificador √∫nico do usu√°rio
    
    if not user_message:
        return "Nenhuma mensagem recebida."

    # Adicionando a mensagem do usu√°rio ao hist√≥rico
    save_history(user_id, user_message, "user")
    
    # Prepara√ß√£o do hist√≥rico para a chamada √† API do OpenAI
    messages = [SYSTEM_PROMPT] + [msg for msg in conversation_history[user_id] if "content" in msg][-10:]  # Mant√©m √∫ltimas 10 mensagens v√°lidas
    
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.5,
            max_tokens=600  # Aumentado para permitir respostas mais completas e evitar cortes
        )
        reply = response["choices"][0]["message"]["content"].strip()
        save_history(user_id, reply, "assistant")
        
        # Aplicar filtragem para evitar recomenda√ß√µes indesejadas
        reply = filter_reply(reply)
    except Exception as e:
        reply = f"Erro ao processar a mensagem: {str(e)}"
    
    return reply

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 5000))
    uvicorn.run(app, host="0.0.0.0", port=port)
