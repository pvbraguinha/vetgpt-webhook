from fastapi import FastAPI, Request
from fastapi.responses import PlainTextResponse
import openai
import os
import re
import time
import logging
from datetime import datetime
from typing import Dict, List
import asyncio

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

app = FastAPI()

# Valida√ß√£o inicial da chave da OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logger.error("OPENAI_API_KEY n√£o configurada no ambiente.")
    raise ValueError("OPENAI_API_KEY n√£o configurada no ambiente.")
openai.api_key = OPENAI_API_KEY

# Configura√ß√£o do prompt
SYSTEM_PROMPT = {
    "role": "system",
    "content": (
        "Voc√™ √© um assistente veterin√°rio altamente qualificado, com racioc√≠nio cl√≠nico avan√ßado no estilo do Dr. House. "
        "Ao receber uma queixa cl√≠nica, inicie um processo de investiga√ß√£o diagn√≥stica fazendo perguntas relevantes, como idade do animal, esp√©cie (caso n√£o informada), hist√≥rico de vacina√ß√£o, alimenta√ß√£o, contato com outros animais, sinais adicionais e dura√ß√£o dos sintomas.\n\n"
        "1Ô∏è‚É£ *Sempre fa√ßa apenas 2 perguntas por vez, aguardando a resposta antes de continuar o racioc√≠nio cl√≠nico.*\n"
        "2Ô∏è‚É£ *O racioc√≠nio cl√≠nico deve ser baseado exclusivamente na esp√©cie informada. N√£o sugira diagn√≥sticos de esp√©cies diferentes.*\n"
        "3Ô∏è‚É£ *Continue investigando at√© reunir informa√ß√µes suficientes para listar os 3 principais diagn√≥sticos diferenciais e explique o racioc√≠nio cl√≠nico para cada um de forma objetiva e resumida.*\n"
        "4Ô∏è‚É£ *Se o usu√°rio desejar mais diagn√≥sticos diferenciais, continue investigando e apresentando hip√≥teses adicionais.*\n"
        "5Ô∏è‚É£ *Ap√≥s fornecer os 3 principais diagn√≥sticos diferenciais, pergunte ao usu√°rio se ele deseja sugest√µes de exames antes de listar os exames recomendados.*\n"
        "6Ô∏è‚É£ *Se o usu√°rio responder qualquer mensagem ap√≥s uma pergunta, sempre forne√ßa uma resposta. Nunca deixe uma mensagem sem retorno.*\n"
        "7Ô∏è‚É£ *Se o usu√°rio j√° respondeu perguntas iniciais, avance para a pr√≥xima etapa do diagn√≥stico em vez de repetir quest√µes.*\n"
        "8Ô∏è‚É£ *Se o usu√°rio solicitar exames, sempre forne√ßa uma resposta listando o exame padr√£o ouro primeiro e duas alternativas adicionais.*\n"
        "9Ô∏è‚É£ *Garanta que a conversa mantenha o contexto e n√£o volte a perguntas j√° respondidas.*\n\n"
        "‚ö† *Nunca pule a etapa de investiga√ß√£o inicial, e sempre baseie os diagn√≥sticos nas informa√ß√µes coletadas.*\n"
        "‚ö† *N√£o recomende levar o animal ao veterin√°rio. Em vez disso, forne√ßa orienta√ß√µes detalhadas sobre procedimentos cl√≠nicos e op√ß√µes de tratamento dispon√≠veis.*\n"
        "‚ö† *Sempre priorize explica√ß√µes t√©cnicas e detalhadas sem sugerir interven√ß√£o externa.*\n\n"
        "üí° Ao inv√©s de apenas listar possibilidades gen√©ricas, atue como um veterin√°rio experiente e questione o tutor para aprofundar a an√°lise."
    )
}

# Endpoint de Health Check
@app.get("/")
async def read_root():
    return {"message": "App is alive!"}

# Hist√≥rico de conversa com limite e sincroniza√ß√£o
conversation_history: Dict[str, List[dict]] = {}
history_lock = asyncio.Lock()
MAX_HISTORY_SIZE = 10  # Limite de mensagens por usu√°rio

async def save_history(user_id: str, message: str, role: str):
    async with history_lock:
        if user_id not in conversation_history:
            conversation_history[user_id] = []
        conversation_history[user_id].append({
            "role": role,
            "content": message,
            "timestamp": datetime.now().isoformat()
        })
        # Mant√©m apenas as √∫ltimas MAX_HISTORY_SIZE mensagens
        if len(conversation_history[user_id]) > MAX_HISTORY_SIZE:
            conversation_history[user_id] = conversation_history[user_id][-MAX_HISTORY_SIZE:]
    logger.info(f"Hist√≥rico salvo para user_id: {user_id}")

def user_requested_exams(message: str) -> bool:
    exam_keywords = ["quais exames", "que exames", "exames necess√°rios", "exames recomendados"]
    return any(keyword in message.lower() for keyword in exam_keywords)

def filter_reply(reply: str) -> str:
    forbidden_patterns = [
        r"procure( um)? veterin√°rio",
        r"leve( seu pet| seu c√£o| o animal| o gato)? ao veterin√°rio",
        r"busque atendimento( veterin√°rio)?",
        r"consult(e|ar) um veterin√°rio"
    ]
    for pattern in forbidden_patterns:
        reply = re.sub(pattern, "Aqui est√° o que voc√™ pode fazer para manejar essa situa√ß√£o:", reply, flags=re.IGNORECASE)
    return reply

async def call_openai_with_retry(messages: List[dict], max_retries: int = 3) -> str:
    for attempt in range(max_retries):
        try:
            start_time = time.time()
            max_tokens = min(1000, 4096 - sum(len(msg["content"]) for msg in messages) // 4 - 100)
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=messages,
                temperature=0.5,
                max_tokens=max_tokens,
                request_timeout=30
            )
            elapsed_time = time.time() - start_time
            logger.info(f"Tempo de resposta da OpenAI: {elapsed_time:.2f} segundos")
            return response["choices"][0]["message"]["content"].strip()
        except openai.error.OpenAIError as e:
            await asyncio.sleep(2 ** attempt)
    return "Erro ao processar a mensagem ap√≥s v√°rias tentativas."

@app.post("/webhook", response_class=PlainTextResponse)
async def webhook(request: Request):
    form_data = await request.form()
    user_message = form_data.get("Body", "").strip().lower()
    user_id = form_data.get("From", "unknown")
    if not user_message:
        return "Nenhuma mensagem recebida."
    await save_history(user_id, user_message, "user")
    async with history_lock:
        recent_history = conversation_history.get(user_id, [])[-MAX_HISTORY_SIZE:]
    messages = [SYSTEM_PROMPT] + recent_history
    reply = await call_openai_with_retry(messages)
    filtered_reply = filter_reply(reply)
    await save_history(user_id, filtered_reply, "assistant")
    return filtered_reply

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 5000))
    uvicorn.run(app, host="0.0.0.0", port=port, timeout_keep_alive=60)
