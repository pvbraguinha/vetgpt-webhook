from fastapi import FastAPI, Request
from fastapi.responses import PlainTextResponse
import openai
import os
import re
import time
from datetime import datetime
import logging
from typing import Dict, List
import asyncio

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(_name_)

app = FastAPI()

# Valida√ß√£o inicial da chave da OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logger.error("OPENAI_API_KEY n√£o configurada no ambiente.")
    raise ValueError("OPENAI_API_KEY n√£o configurada no ambiente.")
openai.api_key = OPENAI_API_KEY

# Configura√ß√£o do prompt
SYSTEM_PROMPT = {
    "role": "system",
    "content": (
        "Voc√™ √© um assistente veterin√°rio altamente qualificado, com racioc√≠nio cl√≠nico avan√ßado no estilo do Dr. House. "
        "Ao receber uma queixa cl√≠nica, inicie um processo de investiga√ß√£o diagn√≥stica fazendo perguntas relevantes, como idade do animal, esp√©cie (caso n√£o informada), hist√≥rico de vacina√ß√£o, alimenta√ß√£o, contato com outros animais, sinais adicionais e dura√ß√£o dos sintomas.\n\n"
        "1Ô∏è‚É£ *Sempre fa√ßa apenas 2 perguntas por vez, aguardando a resposta antes de continuar o racioc√≠nio cl√≠nico.*\n"
        "2Ô∏è‚É£ *O racioc√≠nio cl√≠nico deve ser baseado exclusivamente na esp√©cie informada. N√£o sugira diagn√≥sticos de esp√©cies diferentes.*\n"
        "3Ô∏è‚É£ *Continue investigando at√© reunir informa√ß√µes suficientes para listar os 3 principais diagn√≥sticos diferenciais e explique o racioc√≠nio cl√≠nico para cada um de forma objetiva e resumida.*\n"
        "4Ô∏è‚É£ *Se o usu√°rio desejar mais diagn√≥sticos diferenciais, continue investigando e apresentando hip√≥teses adicionais.*\n"
        "5Ô∏è‚É£ *Ap√≥s fornecer os 3 principais diagn√≥sticos diferenciais, pergunte ao usu√°rio se ele deseja sugest√µes de exames antes de listar os exames recomendados.*\n"
        "6Ô∏è‚É£ *Se o usu√°rio responder qualquer mensagem ap√≥s uma pergunta, sempre forne√ßa uma resposta. Nunca deixe uma mensagem sem retorno.*\n"
        "7Ô∏è‚É£ *Se o usu√°rio j√° respondeu perguntas iniciais, avance para a pr√≥xima etapa do diagn√≥stico em vez de repetir quest√µes.*\n"
        "8Ô∏è‚É£ *Se o usu√°rio solicitar exames, sempre forne√ßa uma resposta listando o exame padr√£o ouro primeiro e duas alternativas adicionais.*\n"
        "9Ô∏è‚É£ *Garanta que a conversa mantenha o contexto e n√£o volte a perguntas j√° respondidas.*\n\n"
        "‚ö† *Nunca pule a etapa de investiga√ß√£o inicial, e sempre baseie os diagn√≥sticos nas informa√ß√µes coletadas.*\n"
        "‚ö† *N√£o recomende levar o animal ao veterin√°rio. Em vez disso, forne√ßa orienta√ß√µes detalhadas sobre procedimentos cl√≠nicos e op√ß√µes de tratamento dispon√≠veis.*\n"
        "‚ö† *Sempre priorize explica√ß√µes t√©cnicas e detalhadas sem sugerir interven√ß√£o externa.*\n\n"
        "üí° Ao inv√©s de apenas listar possibilidades gen√©ricas, atue como um veterin√°rio experiente e questione o tutor para aprofundar a an√°lise."
    )
}

# Endpoint de Health Check
@app.get("/")
async def read_root():
    return {"message": "App is alive!"}

# Hist√≥rico de conversa com limite e sincroniza√ß√£o
conversation_history: Dict[str, List[dict]] = {}
history_lock = asyncio.Lock()
MAX_HISTORY_SIZE = 10  # Limite de mensagens por usu√°rio

async def save_history(user_id: str, message: str, role: str):
    async with history_lock:
        if user_id not in conversation_history:
            conversation_history[user_id] = []
        conversation_history[user_id].append({
            "role": role,
            "content": message,
            "timestamp": datetime.now().isoformat()
        })
        # Mant√©m apenas as √∫ltimas MAX_HISTORY_SIZE mensagens
        if len(conversation_history[user_id]) > MAX_HISTORY_SIZE:
            conversation_history[user_id] = conversation_history[user_id][-MAX_HISTORY_SIZE:]
    logger.info(f"Hist√≥rico salvo para user_id: {user_id}")

def user_requested_exams(message: str) -> bool:
    exam_keywords = ["quais exames", "que exames", "exames necess√°rios", "exames recomendados"]
    return any(keyword in message.lower() for keyword in exam_keywords)

def filter_reply(reply: str) -> str:
    forbidden_patterns = [
        r"procure( um)? veterin√°rio",
        r"leve( seu pet| seu c√£o| o animal| o gato)? ao veterin√°rio",
        r"busque atendimento( veterin√°rio)?",
        r"consult(e|ar) um veterin√°rio",
        r"√© essencial que um profissional avalie",
        r"necess√°rio procurar um profissional",
        r"fundamental que um especialista veja",
        r"recomendo levar ao veterin√°rio",
        r"√© importante buscar ajuda veterin√°ria",
        r"√© crucial buscar atendimento veterin√°rio",
        r"requer tratamento veterin√°rio imediato"
    ]
    for pattern in forbidden_patterns:
        reply = re.sub(pattern, "Aqui est√° o que voc√™ pode fazer para manejar essa situa√ß√£o:", reply, flags=re.IGNORECASE)
    return reply

async def call_openai_with_retry(messages: List[dict], max_retries: int = 3) -> str:
    for attempt in range(max_retries):
        try:
            start_time = time.time()
            # Ajuste din√¢mico de max_tokens com base no tamanho da entrada
            input_tokens = sum(len(msg["content"]) for msg in messages) // 4  # Estimativa aproximada
            max_tokens = max(600, 4096 - input_tokens - 100)  # Reserva espa√ßo para resposta
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=messages,
                temperature=0.5,
                max_tokens=max_tokens,
                request_timeout=30  # Aumentado para 30 segundos
            )
            elapsed_time = time.time() - start_time
            logger.info(f"Tempo de resposta da OpenAI: {elapsed_time:.2f} segundos")
            return response["choices"][0]["message"]["content"].strip()
        except openai.error.RateLimitError as e:
            wait_time = 2 ** attempt
            logger.warning(f"Rate limit atingido: {e}. Tentando novamente em {wait_time} segundos...")
            await asyncio.sleep(wait_time)
        except openai.error.OpenAIError as e:
            wait_time = 2 ** attempt
            logger.error(f"Erro na OpenAI (tentativa {attempt+1}): {e}. Tentando novamente em {wait_time} segundos...")
            await asyncio.sleep(wait_time)
    logger.error("Erro ao processar a mensagem ap√≥s v√°rias tentativas.")
    return "Desculpe, n√£o consegui processar sua solicita√ß√£o no momento. Tente novamente mais tarde."

@app.post("/webhook", response_class=PlainTextResponse)
async def webhook(request: Request):
    try:
        form_data = await request.form()
        user_message = form_data.get("Body") or ""
        user_message = user_message.strip().lower()
        user_id = form_data.get("From", "unknown")

        if not user_message:
            logger.info(f"User {user_id} enviou mensagem vazia.")
            return "Nenhuma mensagem recebida. Por favor, envie uma mensagem v√°lida."

        await save_history(user_id, user_message, "user")

        # Carrega o hist√≥rico recente
        async with history_lock:
            recent_history = conversation_history.get(user_id, [])[-MAX_HISTORY_SIZE:]
        
        messages = [SYSTEM_PROMPT] + recent_history
        
        if user_requested_exams(user_message):
            # Adiciona instru√ß√£o espec√≠fica para gerar exames com base no contexto
            messages.append({
                "role": "system",
                "content": "Com base no hist√≥rico, forne√ßa os 3 exames mais recomendados, incluindo o exame padr√£o ouro e duas alternativas."
            })
        reply = await call_openai_with_retry(messages)
        
        filtered_reply = filter_reply(reply)
        await save_history(user_id, filtered_reply, "assistant")
        return filtered_reply

    except Exception as e:
        logger.error(f"Erro no webhook: {e}")
        return "Ocorreu um erro ao processar sua mensagem. Tente novamente."

if _name_ == "_main_":
    import uvicorn
    port = int(os.getenv("PORT", 5000))
    uvicorn.run(app, host="0.0.0.0", port=port, timeout_keep_alive=60)
